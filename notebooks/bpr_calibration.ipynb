{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tre Torri BPR Calibration Notebook\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This notebook helps estimate per-segment Bureau of Public Roads (BPR) model parameters (\\u03b1, \\u03b2, and capacity) by combining historical Google Routes travel-time samples with peak-hour vehicle counts collected in the field.\n",
        "\n",
        "Steps covered here:\n",
        "1. Load travel-time samples from `data/traffic_samples.jsonl`.\n",
        "2. Load field counts from a separate CSV file.\n",
        "3. Align counts with the corresponding travel-time windows.\n",
        "4. Fit segment-level BPR parameters using non-linear least squares.\n",
        "5. Export the calibrated coefficients so they can be copied into `src/segments.js` (`metadata.flowModel`).\n",
        "\n",
        "> Tip: run the first two code cells (`pip install ...` and the imports) once per notebook kernel start. Subsequent execution should skip repeated installations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: install dependencies into the current environment\n",
        "# Uncomment the next line if pandas/numpy/scipy/matplotlib/tqdm are missing.\n",
        "# !pip install -q pandas numpy scipy matplotlib tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.optimize import least_squares\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "pd.options.display.max_rows = 100\n",
        "pd.options.display.max_columns = 40\n",
        "plt.style.use('seaborn-v0_8')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration\n",
        "Fill in the paths to the samples JSONL file (already part of this repository) and to the field counts CSV you collected. The counts file should include at least the following columns:\n",
        "\n",
        "| column | description |\n",
        "| --- | --- |\n",
        "| `segmentId` | ID used in `src/segments.js` |\n",
        "| `observationStart` | ISO datetime (UTC recommended) for the beginning of the counted interval |\n",
        "| `observationEnd` | ISO datetime for the end of the counted interval |\n",
        "| `observedVehicles` | Number of vehicles counted during the interval |\n",
        "| `capacityGuessVph` *(optional)* | Prior capacity estimate in veh/h (e.g., lanes \u00d7 laneCapacityVph) |\n",
        "\n",
        "The notebook converts `observedVehicles` into veh/h using the interval duration. If you already have veh/h counts, keep `observedVehicles` equal to that value and set `observationEnd = observationStart + 1 hour`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "PROJECT_ROOT = Path.cwd().resolve().parent\n",
        "SAMPLES_PATH = PROJECT_ROOT / 'data' / 'traffic_samples.jsonl'\n",
        "COUNTS_PATH = PROJECT_ROOT / 'data' / 'field_counts.csv'  # update with your counts file\n",
        "OUTPUT_PATH = PROJECT_ROOT / 'data' / 'calibrated_flow_models.csv'\n",
        "\n",
        "print(f'Project root: {PROJECT_ROOT}')\n",
        "print(f'Samples path: {SAMPLES_PATH}')\n",
        "print(f'Counts path:  {COUNTS_PATH}')\n",
        "print(f'Output path:  {OUTPUT_PATH}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load travel-time samples\n",
        "The poller writes JSON Lines where every row contains `durationSeconds`, `staticDurationSeconds`, and optional enrichment fields. We focus on the ratio between live and free-flow travel times.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "samples = pd.read_json(SAMPLES_PATH, lines=True)\n",
        "samples['requestedAt'] = pd.to_datetime(samples['requestedAt'], utc=True)\n",
        "samples = samples.assign(\n",
        "    durationSeconds=samples['durationSeconds'].astype(float),\n",
        "    staticDurationSeconds=samples['staticDurationSeconds'].astype(float),\n",
        ")\n",
        "samples['durationRatio'] = np.where(\n",
        "    (samples['staticDurationSeconds'] > 0) & (samples['durationSeconds'] > 0),\n",
        "    samples['durationSeconds'] / samples['staticDurationSeconds'],\n",
        "    np.nan,\n",
        ")\n",
        "\n",
        "print(f'Total samples loaded: {len(samples):,}')\n",
        "samples.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load field counts\n",
        "The counts CSV can contain multiple observations per segment. Ensure datetimes are parseable; they will be converted to timezone-aware timestamps (UTC).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "counts = pd.read_csv(COUNTS_PATH)\n",
        "datetime_columns = [col for col in ['observationStart', 'observationEnd'] if col in counts.columns]\n",
        "for col in datetime_columns:\n",
        "    counts[col] = pd.to_datetime(counts[col], utc=True)\n",
        "\n",
        "if 'observedVehicles' not in counts.columns:\n",
        "    raise ValueError('Counts CSV must include an `observedVehicles` column (vehicle totals or veh/h).')\n",
        "\n",
        "if 'capacityGuessVph' not in counts.columns:\n",
        "    counts['capacityGuessVph'] = np.nan\n",
        "\n",
        "if {'observationStart', 'observationEnd'}.issubset(counts.columns):\n",
        "    duration_seconds = (counts['observationEnd'] - counts['observationStart']).dt.total_seconds()\n",
        "    counts['observedFlowVph'] = np.where(\n",
        "        duration_seconds > 0,\n",
        "        counts['observedVehicles'] * 3600.0 / duration_seconds,\n",
        "        counts['observedVehicles'],\n",
        "    )\n",
        "else:\n",
        "    print('No observationStart/End columns found; treating `observedVehicles` as veh/h input.')\n",
        "    counts['observedFlowVph'] = counts['observedVehicles']\n",
        "\n",
        "print(f'Field counts loaded: {len(counts):,}')\n",
        "counts.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Align counts with travel-time samples\n",
        "For each field observation, gather the travel-time samples collected within the same interval. You can adjust the `window_pad_minutes` parameter if you need to expand the matching window slightly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "window_pad_minutes = 0  # increase (e.g. 5) if you want to include samples just outside the interval\n",
        "pad = pd.Timedelta(minutes=window_pad_minutes)\n",
        "\n",
        "def summarise_window(segment_id: str, start: pd.Timestamp, end: pd.Timestamp):\n",
        "    mask = (samples['segmentId'] == segment_id)\n",
        "    if pd.notna(start):\n",
        "        mask &= samples['requestedAt'] >= (start - pad)\n",
        "    if pd.notna(end):\n",
        "        mask &= samples['requestedAt'] <= (end + pad)\n",
        "    window = samples[mask]\n",
        "    if window.empty:\n",
        "        return None\n",
        "    ratios = window['durationRatio'].dropna()\n",
        "    if ratios.empty:\n",
        "        return None\n",
        "    return {\n",
        "        'sampleCount': len(window),\n",
        "        'meanRatio': ratios.mean(),\n",
        "        'medianRatio': ratios.median(),\n",
        "        'p90Ratio': ratios.quantile(0.90),\n",
        "        'meanStaticSeconds': window['staticDurationSeconds'].mean(),\n",
        "        'meanDurationSeconds': window['durationSeconds'].mean(),\n",
        "    }\n",
        "\n",
        "aggregated_rows = []\n",
        "for idx, row in tqdm(counts.iterrows(), total=len(counts)):\n",
        "    start = row.get('observationStart', pd.NaT)\n",
        "    end = row.get('observationEnd', pd.NaT)\n",
        "    summary = summarise_window(row['segmentId'], start, end)\n",
        "    if summary is None:\n",
        "        continue\n",
        "    aggregated_rows.append({\n",
        "        'segmentId': row['segmentId'],\n",
        "        'observationStart': start,\n",
        "        'observationEnd': end,\n",
        "        'observedFlowVph': row['observedFlowVph'],\n",
        "        'capacityGuessVph': row.get('capacityGuessVph'),\n",
        "        **summary,\n",
        "    })\n",
        "\n",
        "aligned = pd.DataFrame(aggregated_rows)\n",
        "print(f'Aligned observations: {len(aligned):,} (out of {len(counts):,} field rows)')\n",
        "aligned.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## BPR calibration helpers\n",
        "The calibration routine solves for \\u03b1, \\u03b2, and capacity by minimising the difference between observed travel-time ratios and the BPR model prediction:\n",
        "\\[ r = 1 + \\alpha \\left(\\frac{v}{c}\\right)^\\beta \\]\n",
        "where `r` is the mean ratio, `v` the observed flow (veh/h), and `c` the capacity in veh/h.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calibrate_bpr(observations: pd.DataFrame, initial_alpha: float = 0.15, initial_beta: float = 4.0,\n",
        "                  initial_capacity: float | None = None):\n",
        "    obs = observations.dropna(subset=['meanRatio', 'observedFlowVph']).copy()\n",
        "    obs = obs[obs['meanRatio'] > 1.0]\n",
        "    if obs.empty:\n",
        "        return None\n",
        "\n",
        "    flows = obs['observedFlowVph'].values\n",
        "    ratios = obs['meanRatio'].values\n",
        "    capacity_guess = initial_capacity\n",
        "    if capacity_guess is None:\n",
        "        capacity_guess = np.nanmean(obs.get('capacityGuessVph', np.nan))\n",
        "    if not np.isfinite(capacity_guess) or capacity_guess <= 0:\n",
        "        capacity_guess = flows.max() * 1.25\n",
        "\n",
        "    def residuals(params: np.ndarray) -> np.ndarray:\n",
        "        alpha, beta, capacity = params\n",
        "        if alpha <= 0 or beta <= 0 or capacity <= 0:\n",
        "            return np.ones_like(ratios) * 1e3\n",
        "        modeled = 1.0 + alpha * (flows / capacity) ** beta\n",
        "        return modeled - ratios\n",
        "\n",
        "    x0 = np.array([initial_alpha, initial_beta, capacity_guess], dtype=float)\n",
        "    bounds = (np.array([1e-6, 0.5, 100.0]), np.array([5.0, 12.0, 20000.0]))\n",
        "    result = least_squares(residuals, x0=x0, bounds=bounds)\n",
        "\n",
        "    alpha, beta, capacity = result.x\n",
        "    modeled = 1.0 + alpha * (flows / capacity) ** beta\n",
        "    mae = np.mean(np.abs(modeled - ratios))\n",
        "    rmse = np.sqrt(np.mean((modeled - ratios) ** 2))\n",
        "\n",
        "    return {\n",
        "        'alpha': float(alpha),\n",
        "        'beta': float(beta),\n",
        "        'capacityVph': float(capacity),\n",
        "        'mae': float(mae),\n",
        "        'rmse': float(rmse),\n",
        "        'nObs': int(len(obs)),\n",
        "        'maxObservedFlowVph': float(flows.max()),\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run calibration per segment\n",
        "The loop below processes every segment that has both travel-time data and vehicle counts. Feel free to filter to a subset of segments if you want to focus on specific corridors first.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "calibration_rows = []\n",
        "for segment_id, segment_obs in aligned.groupby('segmentId'):\n",
        "    result = calibrate_bpr(segment_obs)\n",
        "    if result is None:\n",
        "        continue\n",
        "    calibration_rows.append({\n",
        "        'segmentId': segment_id,\n",
        "        **result,\n",
        "    })\n",
        "\n",
        "calibrated = pd.DataFrame(calibration_rows)\n",
        "calibrated.sort_values('rmse', inplace=True)\n",
        "print(f'Segments calibrated: {len(calibrated)}')\n",
        "calibrated\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visual diagnostics\n",
        "Select a segment to inspect observed vs. modelled ratios and flows. This helps confirm that the fitted curve matches your expectations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "segment_to_plot = calibrated['segmentId'].iloc[0] if not calibrated.empty else None\n",
        "if segment_to_plot:\n",
        "    params = calibrated.set_index('segmentId').loc[segment_to_plot]\n",
        "    subset = aligned[aligned['segmentId'] == segment_to_plot]\n",
        "    flows = subset['observedFlowVph'].values\n",
        "    ratios = subset['meanRatio'].values\n",
        "    modeled = 1.0 + params['alpha'] * (flows / params['capacityVph']) ** params['beta']\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(8, 5))\n",
        "    ax.scatter(flows, ratios, color='tab:blue', label='Observed mean ratio')\n",
        "    flow_grid = np.linspace(0, max(flows.max(), params['capacityVph'] * 1.1), 100)\n",
        "    model_grid = 1.0 + params['alpha'] * (flow_grid / params['capacityVph']) ** params['beta']\n",
        "    ax.plot(flow_grid, model_grid, color='tab:red', label='BPR model fit')\n",
        "    ax.set_xlabel('Flow (veh/h)')\n",
        "    ax.set_ylabel('Travel-time ratio')\n",
        "    ax.set_title(f'BPR calibration \u2013 {segment_to_plot}')\n",
        "    ax.legend()\n",
        "    ax.grid(True, which='both', alpha=0.3)\n",
        "else:\n",
        "    print('No calibrated segments available to plot.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Export calibrated coefficients\n",
        "The CSV written below captures the fitted \\u03b1, \\u03b2, and capacity values. Copy the relevant entries into `src/segments.js`, inside each segment's `metadata.flowModel` block (`{ alpha, beta }`). Also consider updating `laneCapacityVph`/`lanes` if the calibrated capacity differs meaningfully from the assumptions used so far.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not calibrated.empty:\n",
        "    OUTPUT_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
        "    calibrated.to_csv(OUTPUT_PATH, index=False)\n",
        "    display(calibrated)\n",
        "    print(f'Calibration table written to: {OUTPUT_PATH}')\n",
        "else:\n",
        "    print('No calibration results to export.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "### Next steps\n",
        "1. Review the `rmse`/`mae` values \u2013 high errors may indicate that the counts do not align with the sampled travel times or that the BPR form is not adequate for that segment.\n",
        "2. Copy the calibrated parameters into `src/segments.js`, for example:\n",
        "   ```js\n",
        "   metadata: {\n",
        "     lanes: 1,\n",
        "     laneCapacityVph: 900,\n",
        "     flowModel: { alpha: 0.27, beta: 3.6 }\n",
        "   }\n",
        "   ```\n",
        "3. Re-run `npm run enrich` so the JSONL dataset picks up the new coefficients.\n",
        "4. Commit the updated `src/segments.js` and the exported calibration CSV (if you want to archive the results).\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}